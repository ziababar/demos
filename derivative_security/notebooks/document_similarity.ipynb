{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document-similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziababar/demos/blob/master/derivative_security/document_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBEUj0wzWt7X",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "---\n",
        "\n",
        "The objective of this notebook program is to find how similar two documents are. This can be used to determine the derivative documents from a source document.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wDcVL_tMSq",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "---\n",
        "The code in this notebook has been adapted from [Compare documents similarity using Python | NLP](https://dev.to/coderasha/compare-documents-similarity-using-python-nlp-4odp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLES_4eKGyjp",
        "colab_type": "text"
      },
      "source": [
        "# Libraries\n",
        "---\n",
        "\n",
        "The following libraries are used in this program.\n",
        "\n",
        "*   **Natural language toolkit:** NLTK contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
        "*   **Gensim:** provides packages for processing texts, working with word vector models.\n",
        "*   **Numpy:** provides packages for numeric processing.\n",
        "*   **Requests:** provides packages for processing making HTTP requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-FnFlOEROOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the required libraries\n",
        "import gensim\n",
        "import nltk\n",
        "import numpy\n",
        "import requests\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LrFc_PDRUCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKOVn2-G8_E",
        "colab_type": "text"
      },
      "source": [
        "# Data Sources\n",
        "---\n",
        "\n",
        "Data sources were generated using https://talktotransformer.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CudznFNuJUF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc1.txt'\n",
        "response = requests.get(target_url)\n",
        "source_doc = response.text\n",
        "print(source_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIfcipsKk2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc2.txt'\n",
        "response = requests.get(target_url)\n",
        "duplicate_doc = response.text\n",
        "print(duplicate_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A4RGLWaKoKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc3.txt'\n",
        "response = requests.get(target_url)\n",
        "partial_doc = response.text\n",
        "print(partial_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lapzvT1ORni0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of documents: \",len(source_doc))\n",
        "print(\"Number of documents: \",len(duplicate_doc))\n",
        "print(\"Number of documents: \",len(partial_doc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4pXI8GHHb7P",
        "colab_type": "text"
      },
      "source": [
        "# Document Parsing\n",
        "---\n",
        "We need to parse the document and extract all the words from the document. This is done through a two step process.\n",
        "1. Open the document and get all the sentences through the sent_tokenize() function.\n",
        "2. For each sentence, get all the words in that sentence using the word_tokenize() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhNFapNIDzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty array that contains all the sentences\n",
        "sent_array = []\n",
        "\n",
        "sent_tokens = sent_tokenize(source_doc)\n",
        "for line in sent_tokens:\n",
        "    sent_array.append(line)\n",
        "\n",
        "print(\"Number of sentences: \", len(sent_array))\n",
        "print(sent_array)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtS81giStyHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_array = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in sent_array]\n",
        "print(word_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYwc5uhm6FbV",
        "colab_type": "text"
      },
      "source": [
        "Gensim requires the words (aka tokens) be converted to unique ids before it can process them.\n",
        "\n",
        "Create a Dictionary object that maps each word to a unique id. Let's convert our sentences to a [list of words] and pass it to the corpora.Dictionary() object. A dictionary maps every word to a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG69oBPR6GPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(word_array)\n",
        "print(dictionary.token2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHZYizEHgJY",
        "colab_type": "text"
      },
      "source": [
        "### Step 1 - Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-26OkEAFRufF",
        "colab_type": "text"
      },
      "source": [
        "Create a Corpus. A ‘corpus’ is typically a ‘collection of documents as a bag of words’.\n",
        "\n",
        "The corpus is an object that contains the word id and its frequency in each document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAl236KY6dZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a corpus and pass the tokenized list of words to the Dictionary.doc2bow()\n",
        "# Here bow stands for bag-of-words\n",
        "corpus_source = [dictionary.doc2bow(word) for word in word_array]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Vtxxpqv32X",
        "colab_type": "text"
      },
      "source": [
        "\"the\" appears two times in second sentence. The has the ID of 12 and its frequency is 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "higIr2xw6r6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus_source)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiKRpSaIask",
        "colab_type": "text"
      },
      "source": [
        "### Step 2 - TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVR8_cD6iwn",
        "colab_type": "text"
      },
      "source": [
        "Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
        "\n",
        "TF-IDF is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length.\n",
        "\n",
        "Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDawqPGSoMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_source = gensim.models.TfidfModel(corpus_source)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP9yMkDn6o0S",
        "colab_type": "text"
      },
      "source": [
        "For example, the word ‘the’ occurs in two documents so it weighted down. The word ‘this’ and 'is' appearing in all three documents so removed altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXnIhVTrwQQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for doc in tfidf_source[corpus_source]:\n",
        "    print([[dictionary[id], numpy.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77yWk1syx2Q_",
        "colab_type": "text"
      },
      "source": [
        "### Step 3 - Parse other documents too\n",
        "Perform the same processing for the other two documents as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QChVVKSx1h4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_array = []\n",
        "sent_tokens = sent_tokenize(duplicate_doc)\n",
        "for line in sent_tokens:\n",
        "    sent_array.append(line)\n",
        "\n",
        "word_array = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in sent_array]\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(word_array)\n",
        "corpus_duplicate = [dictionary.doc2bow(word) for word in word_array]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G443UFM1yu7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_array = []\n",
        "sent_tokens = sent_tokenize(partial_doc)\n",
        "for line in sent_tokens:\n",
        "    sent_array.append(line)\n",
        "\n",
        "word_array = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in sent_array]\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(word_array)\n",
        "corpus_partial = [dictionary.doc2bow(word) for word in word_array]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1OtOUo_ygwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus_source)\n",
        "print(corpus_duplicate)\n",
        "print(corpus_partial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GcziAzIflb",
        "colab_type": "text"
      },
      "source": [
        "# Determining Document Similarity\n",
        "---\n",
        "Now, we are going to create similarity object using cosine similarity. Cosine similarity is a standard measure in Vector Space Modeling to determine the similarity of two vectors.\n",
        "\n",
        "The main class is Similarity, which builds an index for a given set of documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKAOPsWSqs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the index\n",
        "sims = gensim.similarities.MatrixSimilarity(tfidf_source[corpus_source])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvtllxKJKSO",
        "colab_type": "text"
      },
      "source": [
        "To determine similarity between two documents, we perform two steps. First we get a query document based on the document that needs to be compared, and this is then used to get the similarity index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNERDHZTXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain a similarity query against the source corpus\n",
        "query_duplicate = tfidf_source[corpus_duplicate]\n",
        "query_partial = tfidf_source[corpus_partial]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlGNs17Z3eUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the similarity index for each of the documents\n",
        "print(numpy.around(sims[query_duplicate], decimals=2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V7F2QJK3e-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(numpy.around(sims[query_partial], decimals=2))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}