{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document-similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziababar/demos/blob/master/derivative_security/document_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBEUj0wzWt7X",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "The objective of this notebook program is to find how similar two documents are. This can be used to determine the derivative documents from a source document.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLES_4eKGyjp",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFhxliRRPm4",
        "colab_type": "text"
      },
      "source": [
        "The following libraries are used in this program.\n",
        "\n",
        "**Scikit-learn**\n",
        "\n",
        "**Natural language toolkit:** NLTK contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
        "\n",
        "**Gensim:** provides packages for processing texts, working with word vector models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-FnFlOEROOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the required libraries\n",
        "import gensim\n",
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "import requests\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LrFc_PDRUCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "08e25452-5fd4-4070-abe5-4c24412b8cec"
      },
      "source": [
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI79I4YHRZVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(dir(gensim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKOVn2-G8_E",
        "colab_type": "text"
      },
      "source": [
        "# Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CudznFNuJUF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "377f0805-841d-4bf2-c8b4-ba6ecf652c35"
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc1.txt'\n",
        "response = requests.get(target_url)\n",
        "doc1 = response.text\n",
        "print(doc1)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Pentagon must stop pretending the state of cyberwarfare is something new, and that the damage it wreaks is different. If we have not absorbed the lessons of the past, then they will be tested on us again.\n",
            "\n",
            "Anonymous \"Anonymous\" is the hacker group famous for its attacks on the Church of Scientology, international terrorist groups, and other governments and corporations. As the name implies, the collective can appear anywhere at any time, and offers its services as a collective attack on Internet services.\n",
            "\n",
            "In lieu of completely replacing or replacing the process of wargaming the point I want to bring up is that wargames will continue to exist and wargames can be designed and tested. In the case of Dark Age of Camelot I had some limitations, one of which was the lack of people online to play with. I was a solo design, purely with a notebook. And as you know most games I make end up being played by others. And this means that when my friends started playing I was there, and now I can go to Wargames Monday and Google groups. Also, you don't have to make my dream come true to play.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIfcipsKk2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc2.txt'\n",
        "response = requests.get(target_url)\n",
        "doc2 = response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A4RGLWaKoKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc3.txt'\n",
        "response = requests.get(target_url)\n",
        "doc3 = response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greBaHqGRfw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_documents = [\"Someone I know recently combined Maple Syrup & buttered Popcorn thinking it would taste like caramel popcorn. It didn’t and they don’t recommend anyone else do it either.\",\n",
        "                 \"Sometimes it is better to just walk away from things and go back to them later when you’re in a better frame of mind.\",\n",
        "                 \"Italy is my favorite country; in fact, I plan to spend two weeks there next year.\",\n",
        "                 \"He turned in the research paper on Friday; otherwise, he would have not passed the class.\",\n",
        "                 \"Keep up with evolving privacy and security regulations across all industries with monitoring and detailed reporting from encryption status to failed authentication and everything in between. \"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lapzvT1ORni0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b2a59fb3-efb6-473e-8f47-ce81e079305c"
      },
      "source": [
        "print(\"Number of documents:\",len(raw_documents))\n",
        "print(\"Number of documents:\",len(doc1))\n",
        "print(\"Number of documents:\",len(doc2))\n",
        "print(\"Number of documents:\",len(doc3))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 5\n",
            "Number of documents: 1098\n",
            "Number of documents: 1098\n",
            "Number of documents: 514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4pXI8GHHb7P",
        "colab_type": "text"
      },
      "source": [
        "# Document Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBg1Mdp3Rp_G",
        "colab_type": "text"
      },
      "source": [
        "**Open document 1**\n",
        "\n",
        "**Open document 2**\n",
        "\n",
        "**Open document 3**\n",
        "\n",
        "\n",
        "need to review them too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZZ8NC6YIHyj",
        "colab_type": "text"
      },
      "source": [
        "Create a .txt file and write 4-5 sentences in it. Include the file with the same directory of your Python program. Now, we are going to open this file with Python and split sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhNFapNIDzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0b54095-b6bb-4cd6-b453-0a56356dbe27"
      },
      "source": [
        "file_docs = []\n",
        "\n",
        "# with open ('demofile.txt') as f:\n",
        "#    tokens = sent_tokenize(f.read())\n",
        "#    for line in tokens:\n",
        "#        file_docs.append(line)\n",
        "\n",
        "tokens = sent_tokenize(doc1)\n",
        "for line in tokens:\n",
        "    file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHZYizEHgJY",
        "colab_type": "text"
      },
      "source": [
        "### Method 1 - Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-26OkEAFRufF",
        "colab_type": "text"
      },
      "source": [
        "We use the method word_tokenize() to split a sentence into words. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbzjw93DIPYa",
        "colab_type": "text"
      },
      "source": [
        "Once we added tokenized sentences in array, it is time to tokenize words for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn3Hm6MHRjMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(file_docs)] \n",
        "            for text in raw_documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLN-U2NuRziU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(gen_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTVTaV3mRrcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "#print(dictionary[5])\n",
        "#print(dictionary.token2id['road'])\n",
        "#print(\"Number of words in dictionary:\",len(dictionary))\n",
        "#for i in range(len(dictionary)):\n",
        "#    print(i, dictionary[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POqBiZMUSkVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "#print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiKRpSaIask",
        "colab_type": "text"
      },
      "source": [
        "### Method 2 - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDawqPGSoMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "#print(tf_idf)\n",
        "s = 0\n",
        "for i in corpus:\n",
        "    s += len(i)\n",
        "#print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GcziAzIflb",
        "colab_type": "text"
      },
      "source": [
        "# Determining Document Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT2f_klUInRF",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKAOPsWSqs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sims = gensim.similarities.Similarity('/usr',tf_idf[corpus], num_features=len(dictionary))\n",
        "#print(sims)\n",
        "#print(type(sims))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN5V3fojItQM",
        "colab_type": "text"
      },
      "source": [
        "**Create Query Document**\n",
        "\n",
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. So, create second .txt file which will include query documents or sentences and tokenize them as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNJ9UQCSrgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_1 = [w.lower() for w in word_tokenize(\"Evolving privacy and security regulations.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_1)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPBfbDhJIyo",
        "colab_type": "text"
      },
      "source": [
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvtllxKJKSO",
        "colab_type": "text"
      },
      "source": [
        "**Document similarities to query**\n",
        "\n",
        "At this stage, you will see similarities between the query and all index documents. To obtain similarities of our query document against the indexed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNERDHZTXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_2 = [w.lower() for w in word_tokenize(\"Data encrytion is important for enterprises looking for data security and data privacy.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_2)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WIt4SP_Svta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_3 = [w.lower() for w in word_tokenize(\"He turned in the research paper on Friday; otherwise, he would have not passed the class.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_3)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydVUueBBKP0r",
        "colab_type": "text"
      },
      "source": [
        "**Computing Cosine Similarity using scikit-learn**\n",
        "\n",
        "Generally a cosine similarity between two documents is used as a similarity measure of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4db1K9Kfat",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQo9wNxKOrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [open(f) for f in text_files]\n",
        "tfidf = TfidfVectorizer().fit_transform(documents)\n",
        "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpd9aRAjKZV9",
        "colab_type": "text"
      },
      "source": [
        ">>> corpus = [\"I'd like an apple\", \n",
        "...           \"An apple a day keeps the doctor away\", \n",
        "...           \"Never compare an apple to an orange\", \n",
        "...           \"I prefer scikit-learn to Orange\", \n",
        "...           \"The scikit-learn docs are Orange and Blue\"]        \n",
        ">>> vect = TfidfVectorizer(min_df=1, stop_words=\"english\")    >>> tfidf = vect.fit_transform(corpus)                        >>> pairwise_similarity = tfidf * tfidf.T "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-z9cTeK6fW",
        "colab_type": "text"
      },
      "source": [
        "**DOT PRODUCT** method"
      ]
    }
  ]
}