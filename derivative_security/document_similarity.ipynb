{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document-similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziababar/demos/blob/master/derivative_security/document_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBEUj0wzWt7X",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "The objective of this notebook program is to find how similar two documents are. This can be used to determine the derivative documents from a source document.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLES_4eKGyjp",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFhxliRRPm4",
        "colab_type": "text"
      },
      "source": [
        "The following libraries are used in this program.\n",
        "\n",
        "**Scikit-learn**\n",
        "\n",
        "**Natural language toolkit:** NLTK contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
        "\n",
        "**Gensim:** provides packages for processing texts, working with word vector models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-FnFlOEROOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the required libraries\n",
        "import gensim\n",
        "import nltk\n",
        "import sklearn\n",
        "\n",
        "import requests\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LrFc_PDRUCT",
        "colab_type": "code",
        "outputId": "08e25452-5fd4-4070-abe5-4c24412b8cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI79I4YHRZVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(dir(gensim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKOVn2-G8_E",
        "colab_type": "text"
      },
      "source": [
        "# Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CudznFNuJUF7",
        "colab_type": "code",
        "outputId": "377f0805-841d-4bf2-c8b4-ba6ecf652c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc1.txt'\n",
        "response = requests.get(target_url)\n",
        "doc1 = response.text\n",
        "print(doc1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Pentagon must stop pretending the state of cyberwarfare is something new, and that the damage it wreaks is different. If we have not absorbed the lessons of the past, then they will be tested on us again.\n",
            "\n",
            "Anonymous \"Anonymous\" is the hacker group famous for its attacks on the Church of Scientology, international terrorist groups, and other governments and corporations. As the name implies, the collective can appear anywhere at any time, and offers its services as a collective attack on Internet services.\n",
            "\n",
            "In lieu of completely replacing or replacing the process of wargaming the point I want to bring up is that wargames will continue to exist and wargames can be designed and tested. In the case of Dark Age of Camelot I had some limitations, one of which was the lack of people online to play with. I was a solo design, purely with a notebook. And as you know most games I make end up being played by others. And this means that when my friends started playing I was there, and now I can go to Wargames Monday and Google groups. Also, you don't have to make my dream come true to play.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACIfcipsKk2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc2.txt'\n",
        "response = requests.get(target_url)\n",
        "doc2 = response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A4RGLWaKoKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_url = 'https://raw.githubusercontent.com/ziababar/demos/master/derivative_security/data/doc3.txt'\n",
        "response = requests.get(target_url)\n",
        "doc3 = response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greBaHqGRfw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_documents = [\"Someone I know recently combined Maple Syrup & buttered Popcorn thinking it would taste like caramel popcorn. It didn’t and they don’t recommend anyone else do it either.\",\n",
        "                 \"Sometimes it is better to just walk away from things and go back to them later when you’re in a better frame of mind.\",\n",
        "                 \"Italy is my favorite country; in fact, I plan to spend two weeks there next year.\",\n",
        "                 \"He turned in the research paper on Friday; otherwise, he would have not passed the class.\",\n",
        "                 \"Keep up with evolving privacy and security regulations across all industries with monitoring and detailed reporting from encryption status to failed authentication and everything in between. \"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lapzvT1ORni0",
        "colab_type": "code",
        "outputId": "b2a59fb3-efb6-473e-8f47-ce81e079305c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Number of documents:\",len(raw_documents))\n",
        "print(\"Number of documents:\",len(doc1))\n",
        "print(\"Number of documents:\",len(doc2))\n",
        "print(\"Number of documents:\",len(doc3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 5\n",
            "Number of documents: 1098\n",
            "Number of documents: 1098\n",
            "Number of documents: 514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4pXI8GHHb7P",
        "colab_type": "text"
      },
      "source": [
        "# Document Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBg1Mdp3Rp_G",
        "colab_type": "text"
      },
      "source": [
        "**Open document 1**\n",
        "\n",
        "**Open document 2**\n",
        "\n",
        "**Open document 3**\n",
        "\n",
        "\n",
        "need to review them too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvZMF0I5v1A",
        "colab_type": "text"
      },
      "source": [
        "We use the method word_tokenize() to split a sentence into words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAxT7bjj5vJv",
        "colab_type": "text"
      },
      "source": [
        "We need to count average words per sentence, so for accomplishing such a task, we use sentence tokenization as well as words to calculate the ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_rTH28O5pbE",
        "colab_type": "text"
      },
      "source": [
        "Program will open file and read it's content. Then it will add tokenized sentences into the array for word tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhNFapNIDzK",
        "colab_type": "code",
        "outputId": "e0b54095-b6bb-4cd6-b453-0a56356dbe27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file_docs = []\n",
        "\n",
        "tokens = sent_tokenize(doc1)\n",
        "for line in tokens:\n",
        "    file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "836BrtnG5148",
        "colab_type": "text"
      },
      "source": [
        "Once we added tokenized sentences in array, it is time to tokenize words for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQnlxWmj53yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in file_docs]\n",
        "\n",
        "print(gen_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYwc5uhm6FbV",
        "colab_type": "text"
      },
      "source": [
        "In order to work on text documents, Gensim requires the words (aka tokens) be converted to unique ids. So, Gensim lets you create a Dictionary object that maps each word to a unique id. Let's convert our sentences to a [list of words] and pass it to the corpora.Dictionary() object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG69oBPR6GPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "print(dictionary.token2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqq9rD2U6QdA",
        "colab_type": "text"
      },
      "source": [
        "A dictionary maps every word to a number. Gensim lets you read the text and update the dictionary, one line at a time, without loading the entire text file into system memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHZYizEHgJY",
        "colab_type": "text"
      },
      "source": [
        "### Method 1 - Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-26OkEAFRufF",
        "colab_type": "text"
      },
      "source": [
        "The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence).\n",
        "\n",
        "Note that, a ‘token’ typically means a ‘word’. A ‘document’ can typically refer to a ‘sentence’ or ‘paragraph’ and a ‘corpus’ is typically a ‘collection of documents as a bag of words’.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nriMOHl6bfk",
        "colab_type": "text"
      },
      "source": [
        "Now, create a bag of words corpus and pass the tokenized list of words to the Dictionary.doc2bow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAl236KY6dZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "higIr2xw6r6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3K2ESvB6rEM",
        "colab_type": "text"
      },
      "source": [
        "As you see we used \"the\" two times in second sentence and if you look word with id=12 (the) you will see that its frequency is 2 (appears 2 times in sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiKRpSaIask",
        "colab_type": "text"
      },
      "source": [
        "### Method 2 - TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZVR8_cD6iwn",
        "colab_type": "text"
      },
      "source": [
        "Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.\n",
        "\n",
        "Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDawqPGSoMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tfidf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP9yMkDn6o0S",
        "colab_type": "text"
      },
      "source": [
        "The word ‘the’ occurs in two documents so it weighted down. The word ‘this’ and 'is' appearing in all three documents so removed altogether."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GcziAzIflb",
        "colab_type": "text"
      },
      "source": [
        "# Determining Document Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT2f_klUInRF",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKAOPsWSqs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the index\n",
        " sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN5V3fojItQM",
        "colab_type": "text"
      },
      "source": [
        "**Create Query Document**\n",
        "\n",
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. So, create second .txt file which will include query documents or sentences and tokenize them as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNJ9UQCSrgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2_docs = []\n",
        "\n",
        "with open ('demofile2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
        "create bag of words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPBfbDhJIyo",
        "colab_type": "text"
      },
      "source": [
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvtllxKJKSO",
        "colab_type": "text"
      },
      "source": [
        "**Document similarities to query**\n",
        "\n",
        "At this stage, you will see similarities between the query and all index documents. To obtain similarities of our query document against the indexed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNERDHZTXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "# print(document_number, document_similarity)\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnVSD6J97TqU",
        "colab_type": "text"
      },
      "source": [
        "Cosine measure returns similarities in the range (the greater, the more similar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydVUueBBKP0r",
        "colab_type": "text"
      },
      "source": [
        "**Computing Cosine Similarity using scikit-learn**\n",
        "\n",
        "Generally a cosine similarity between two documents is used as a similarity measure of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4db1K9Kfat",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQo9wNxKOrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [open(f) for f in text_files]\n",
        "tfidf = TfidfVectorizer().fit_transform(documents)\n",
        "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpd9aRAjKZV9",
        "colab_type": "text"
      },
      "source": [
        ">>> corpus = [\"I'd like an apple\", \n",
        "...           \"An apple a day keeps the doctor away\", \n",
        "...           \"Never compare an apple to an orange\", \n",
        "...           \"I prefer scikit-learn to Orange\", \n",
        "...           \"The scikit-learn docs are Orange and Blue\"]        \n",
        ">>> vect = TfidfVectorizer(min_df=1, stop_words=\"english\")    >>> tfidf = vect.fit_transform(corpus)                        >>> pairwise_similarity = tfidf * tfidf.T "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-z9cTeK6fW",
        "colab_type": "text"
      },
      "source": [
        "**DOT PRODUCT** method"
      ]
    }
  ]
}